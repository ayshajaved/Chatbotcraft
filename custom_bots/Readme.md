# Training an E-Commerce Chatbot Model on Custom Data

This project guides you through training a chatbot model on custom data for an e-commerce customer service use case. It uses **PyTorch** and **Hugging Face Transformers** to fine-tune **DialoGPT-small**, with **TensorFlow** for an alternative seq2seq approach and **scikit-learn** for evaluation metrics. The process includes data preparation, preprocessing with NLP techniques, model training, fine-tuning, evaluation, and deployment preparation.

## Purpose
The goal is to train a chatbot that answers e-commerce queries (e.g., product prices, return policies) using a custom dataset. The chatbot leverages transfer learning by fine-tuning a pre-trained model, ensuring high-quality responses with minimal data.

## Prerequisites
- **Python**: Version 3.8 or higher.
- **Hardware**: GPU recommended (e.g., NVIDIA CUDA-compatible); CPU viable for small datasets.
- **Dataset**: A JSON file with input-response pairs (e.g., `data.json`).
- **Knowledge**: Basic understanding of Python, NLP, and machine learning.

## Required Libraries
| Library | Purpose | Installation Command |
|---------|---------|----------------------|
| `transformers` | Pre-trained models (DialoGPT, T5) and tokenizers. | `pip install transformers` |
| `torch` | PyTorch for model training and inference. | `pip install torch` |
| `tensorflow` | Alternative framework for seq2seq models. | `pip install tensorflow` |
| `datasets` | Manage and preprocess datasets. | `pip install datasets` |
| `pandas` | Data loading and manipulation. | `pip install pandas` |
| `spacy` | NLP preprocessing (lemmatization, tokenization). | `pip install spacy; python -m spacy download en_core_web_sm` |
| `nltk` | Text processing and tokenization. | `pip install nltk` |
| `scikit-learn` | Evaluation metrics (e.g., BLEU score). | `pip install scikit-learn` |
| `fastapi` | Optional for deployment. | `pip install fastapi uvicorn` |

Install all dependencies:
```bash
pip install transformers torch tensorflow datasets pandas spacy nltk scikit-learn fastapi uvicorn
python -m spacy download en_core_web_sm
python -c "import nltk; nltk.download('punkt')"
```

## Dataset Preparation
Create a JSON file (`data.json`) with conversation pairs:
```json
[
  {"input": "What is the price of the blue shirt?", "response": "The blue shirt costs $29.99."},
  {"input": "How do I return an item?", "response": "To return an item, visit our website and fill out the returns form within 30 days."},
  ...
]
```
- **Sources**: Customer support logs, FAQs, or synthetic data (e.g., generated by GPT-4).
- **Size**: 1,000–10,000 pairs recommended; split into 80% training, 10% validation, 10% test.
- **Cleaning**: Remove duplicates, normalize text (lowercase, remove special characters).

## Steps to Train the Model

### Step 1: Set Up the Environment
1. Create a project directory:
   ```bash
   mkdir ecommerce_chatbot_training
   cd ecommerce_chatbot_training
   ```
2. Set up a virtual environment:
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```
3. Install libraries (see above).

### Step 2: Preprocess the Data
- Load the dataset using Pandas.
- Clean and normalize text using spaCy (lemmatization, lowercase).
- Tokenize for DialoGPT using its tokenizer, adding special tokens (`</s>`).

### Step 3: Fine-Tune DialoGPT with PyTorch
- Load the pre-trained DialoGPT-small model.
- Fine-tune on your dataset using Hugging Face’s `Trainer` API.
- Save the fine-tuned model.

### Step 4: Alternative Approach with TensorFlow (Seq2Seq)
For smaller datasets or simpler tasks, use a TensorFlow-based seq2seq model with LSTM:
- Encode input/output texts as integer sequences.
- Build an encoder-decoder model.
- Train with categorical cross-entropy loss.

### Step 5: Evaluate the Model
- **Perplexity**: Measure using `trainer.evaluate()` (lower is better).
- **BLEU Score**: Compare generated responses to references using scikit-learn and NLTK:
  ```python
  from nltk.translate.bleu_score import sentence_bleu
  reference = "The blue shirt costs $29.99.".split()
  generated = "The blue shirt is priced at $29.99.".split()
  print(sentence_bleu([reference], generated))
  ```
- **Human Evaluation**: Test with sample queries.

### Step 6: Deploy the Model
- Use FastAPI to create an API endpoint (see README 1).
- Integrate with platforms like Telegram or Slack.

## Code
Below is the implementation of `train_chatbot.py` for fine-tuning DialoGPT with PyTorch and evaluating with scikit-learn:

```python
import json
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments
from datasets import Dataset
import pandas as pd
import spacy
from nltk.tokenize import word_tokenize
import nltk
from sklearn.metrics import accuracy_score
from nltk.translate.bleu_score import sentence_bleu

# Initialize NLTK and spaCy
nltk.download('punkt')
nlp = spacy.load("en_core_web_sm")

# Step 1: Load and preprocess dataset
def load_dataset(file_path):
    with open(file_path, 'r') as f:
        data = json.load(f)
    df = pd.DataFrame(data)
    df['input'] = df['input'].apply(lambda x: ' '.join([token.lemma_.lower() for token in nlp(x)]))
    df['response'] = df['response'].apply(lambda x: ' '.join([token.lemma_.lower() for token in nlp(x)]))
    return Dataset.from_pandas(df)

# Step 2: Tokenize data for DialoGPT
def preprocess_data(dataset, tokenizer):
    def tokenize_function(examples):
        inputs = [f"{inp} </s> {resp} </s>" for inp, resp in zip(examples['input'], examples['response'])]
        model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding='max_length')
        model_inputs['labels'] = model_inputs['input_ids'].copy()
        return model_inputs
    return dataset.map(tokenize_function, batched=True)

# Step 3: Load model and tokenizer
model_name = "microsoft/DialoGPT-small"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# Step 4: Load and preprocess dataset
dataset = load_dataset("data.json")
tokenized_dataset = preprocess_data(dataset, tokenizer)

# Step 5: Set up training arguments
training_args = TrainingArguments(
    output_dir="./ecommerce_chatbot_model",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    warmup_steps=100,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=10,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
)

# Step 6: Train the model
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    eval_dataset=tokenized_dataset,  # Use separate validation set in practice
)
trainer.train()

# Save the model
model.save_pretrained("./ecommerce_chatbot_model")
tokenizer.save_pretrained("./ecommerce_chatbot_model")

# Step 7: Inference function
def generate_response(user_input, max_length=50):
    processed_input = ' '.join([token.lemma_.lower() for token in nlp(user_input)])
    input_ids = tokenizer.encode(processed_input + tokenizer.eos_token, return_tensors='pt')
    output = model.generate(input_ids, max_length=max_length, pad_token_id=tokenizer.eos_token_id)
    return tokenizer.decode(output[0], skip_special_tokens=True).split('</s>')[-1].strip()

# Step 8: Evaluate with BLEU score
def evaluate_bleu(reference, generated):
    return sentence_bleu([reference.split()], generated.split())

# Example evaluation
test_input = "What is the price of the blue shirt?"
reference = "The blue shirt costs $29.99."
generated = generate_response(test_input)
bleu_score = evaluate_bleu(reference, generated)
print(f"BLEU Score: {bleu_score}")

# Step 9: Optional TensorFlow Seq2Seq Model
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, LSTM, Dense
import numpy as np

def build_seq2seq_model(vocab_size, max_length):
    # Encoder
    encoder_inputs = Input(shape=(max_length,))
    encoder = LSTM(256, return_state=True)
    encoder_outputs, state_h, state_c = encoder(encoder_inputs)
    encoder_states = [state_h, state_c]
    
    # Decoder
    decoder_inputs = Input(shape=(max_length,))
    decoder_lstm = LSTM(256, return_sequences=True, return_state=True)
    decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)
    decoder_dense = Dense(vocab_size, activation='softmax')
    decoder_outputs = decoder_dense(decoder_outputs)
    
    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)
    return model

# Example usage (simplified)
# seq2seq_model = build_seq2seq_model(vocab_size=10000, max_length=128)
# seq2seq_model.compile(optimizer='adam', loss='categorical_crossentropy')
# seq2seq_model.fit([encoder_input_data, decoder_input_data], decoder_target_data, epochs=10)
```

## Usage
1. Save the code as `train_chatbot.py`.
2. Create `data.json` with your dataset.
3. Run the script: `python train_chatbot.py`.
4. Evaluate the model using the BLEU score or test interactively:
   ```python
   print(generate_response("How do I return an item?"))
   ```
5. For TensorFlow seq2seq, preprocess data into integer sequences and uncomment the seq2seq section.

## Customization
- **Model**: Use T5 (`google/t5-small`) for Q&A tasks or BlenderBot for empathetic dialogue.
- **Hyperparameters**: Adjust `num_train_epochs`, `batch_size`, or `learning_rate` based on dataset size.
- **Evaluation**: Add ROUGE scores or human evaluation.
- **Deployment**: See README 1 for API deployment.

## Troubleshooting
- **Out of Memory**: Reduce `batch_size` or use a smaller model (e.g., DistilBERT).
- **Poor Performance**: Increase dataset size or fine-tune for more epochs.
- **TensorFlow Errors**: Ensure compatible versions (`tensorflow>=2.10`).

## Future Improvements
- Use LoRA for efficient fine-tuning.
- Implement RAG for accurate responses.
- Add context management for multi-turn conversations.